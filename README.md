# From Bag-of-Words to Transformers on a Tiny Vocabulary

This project explores the evolution of Natural Language Processing models by building and comparing classical and modern architectures from scratch on a minimal synthetic dataset.

## Models implemented

- Bag-of-Words linear classifier
- Embedding-based Feedforward Neural Network
- Recurrent Neural Network (RNN)
- Transformer (single-head self-attention)

## Goals

- Understand how language models work internally
- Compare representational power across architectures
- Demonstrate how complexity interacts with dataset size
- Provide an educational, fully transparent implementation

## Dataset

A tiny synthetic corpus designed to encode simple semantic rules involving preservation, destruction, and negation.

## Requirements

- Python 3.x
- PyTorch
- Jupyter Notebook

## License

MIT
