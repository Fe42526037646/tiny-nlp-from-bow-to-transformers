{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cbba40a-10a2-4e4a-b2ee-9f7d07cb81ad",
   "metadata": {},
   "source": [
    "# From Bag-of-Words to Transformers: Building NLP Models from Scratch on a Tiny Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484208a-7fb6-4766-9a21-fded7639b69a",
   "metadata": {},
   "source": [
    "## Libraries and style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e37b07c2-a969-4938-8802-cdd838109a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce298a21-1df2-4022-8486-1815d39c8437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "/* =========Global notebook styling====== */\n",
       "body {font-family: \"Inter\", \"Helvetica Neue\", Arial, sans-serif;background-color: #fafafa;color: #2f2f2f;line-height: 1.65;}\n",
       "\n",
       "/* =========H1 — Major Sections========== */\n",
       "h1 {font-family: \"Georgia\", \"Times New Roman\", serif;font-size: 2.2em;font-weight: 600;color: #2c3e50;background: linear-gradient(90deg, #e8f0f2, #f7f9fa);padding: 16px 20px;border-radius: 12px;margin-top: 2.2em;margin-bottom: 1.2em;border-left: 8px solid #a8c3d6;}\n",
       "\n",
       "/* =========H2 — Subsections============= */\n",
       "h2 {font-family: \"Georgia\", \"Times New Roman\", serif;font-size: 1.6em;font-weight: 500;color: #34495e;margin-top: 2em;margin-bottom: 0.8em;padding-left: 14px;border-left: 4px solid #c7dce8;}\n",
       "\n",
       "/* =========H3 — Minor headings=========== */\n",
       "h3 {font-size: 1.2em;font-weight: 600;color: #4a6572;margin-top: 1.4em;margin-bottom: 0.6em;}\n",
       "\n",
       "/* =========Paragraphs==================== */\n",
       "p {font-size: 1.02em;margin-bottom: 0.9em;}\n",
       "\n",
       "/* =========Inline math & code============ */\n",
       "code {background-color: #f1f3f4;color: #37474f;padding: 2px 6px;border-radius: 6px;font-size: 0.95em;}\n",
       "\n",
       "/* =========Math blocks=================== */\n",
       ".MathJax_Display {background: #f7f9fb;padding: 14px;border-radius: 10px;border-left: 4px solid #d6e4ec;}\n",
       "\n",
       "/* =========Lists========================= */\n",
       "ul li {margin-bottom: 0.4em;}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "/* =========Global notebook styling====== */\n",
    "body {font-family: \"Inter\", \"Helvetica Neue\", Arial, sans-serif;background-color: #fafafa;color: #2f2f2f;line-height: 1.65;}\n",
    "    \n",
    "/* =========H1 — Major Sections========== */\n",
    "h1 {font-family: \"Georgia\", \"Times New Roman\", serif;font-size: 2.2em;font-weight: 600;color: #2c3e50;background: linear-gradient(90deg, #e8f0f2, #f7f9fa);padding: 16px 20px;border-radius: 12px;margin-top: 2.2em;margin-bottom: 1.2em;border-left: 8px solid #a8c3d6;}\n",
    "    \n",
    "/* =========H2 — Subsections============= */\n",
    "h2 {font-family: \"Georgia\", \"Times New Roman\", serif;font-size: 1.6em;font-weight: 500;color: #34495e;margin-top: 2em;margin-bottom: 0.8em;padding-left: 14px;border-left: 4px solid #c7dce8;}\n",
    "\n",
    "/* =========H3 — Minor headings=========== */\n",
    "h3 {font-size: 1.2em;font-weight: 600;color: #4a6572;margin-top: 1.4em;margin-bottom: 0.6em;}\n",
    "\n",
    "/* =========Paragraphs==================== */\n",
    "p {font-size: 1.02em;margin-bottom: 0.9em;}\n",
    "\n",
    "/* =========Inline math & code============ */\n",
    "code {background-color: #f1f3f4;color: #37474f;padding: 2px 6px;border-radius: 6px;font-size: 0.95em;}\n",
    "\n",
    "/* =========Math blocks=================== */\n",
    ".MathJax_Display {background: #f7f9fb;padding: 14px;border-radius: 10px;border-left: 4px solid #d6e4ec;}\n",
    "\n",
    "/* =========Lists========================= */\n",
    "ul li {margin-bottom: 0.4em;}\n",
    "\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f35e2b-ca5a-464c-bbb3-442ea35e5656",
   "metadata": {},
   "source": [
    "## Setup and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95a3c52f-3d6b-4699-8056-db1b8bc28bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ['<BOS>',\n",
       "  '<EOS>',\n",
       "  'human',\n",
       "  'nature',\n",
       "  'consume',\n",
       "  'preserve',\n",
       "  'destroy',\n",
       "  'not',\n",
       "  'sustain',\n",
       "  'collapse'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "V = [\"<BOS>\", \"<EOS>\", \"human\", \"nature\", \"consume\", \"preserve\", \"destroy\", \"not\", \"sustain\", \"collapse\"]\n",
    "stoi = {t:i for i,t in enumerate(V)}\n",
    "itos = {i:t for t,i in stoi.items()}\n",
    "vocab_size = len(V)\n",
    "\n",
    "def encode(tokens):\n",
    "    return [stoi[t] for t in tokens]\n",
    "\n",
    "def decode(ids):\n",
    "    return [itos[i] for i in ids]\n",
    "\n",
    "vocab_size, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34b3ff-3423-4a05-9b43-6edd8fa1fb1c",
   "metadata": {},
   "source": [
    "## Dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6729314b-0d74-4039-827a-6a4ba067d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'human', 'preserve', 'nature', '<EOS>'] -> sustain\n",
      "['<BOS>', 'human', 'not', 'preserve', 'nature', '<EOS>'] -> collapse\n",
      "['<BOS>', 'human', 'destroy', 'nature', '<EOS>'] -> collapse\n",
      "['<BOS>', 'human', 'preserve', 'nature', '<EOS>'] -> sustain\n",
      "['<BOS>', 'human', 'preserve', 'nature', '<EOS>'] -> sustain\n",
      "['<BOS>', 'human', 'consume', 'nature', '<EOS>'] -> collapse\n",
      "['<BOS>', 'human', 'not', 'preserve', 'nature', '<EOS>'] -> collapse\n",
      "['<BOS>', 'human', 'consume', 'nature', '<EOS>'] -> collapse\n"
     ]
    }
   ],
   "source": [
    "def sample_sentence():\n",
    "    action = random.choice([\"consume\", \"preserve\", \"destroy\"])\n",
    "    use_not = random.random() < 0.35  \n",
    "    tokens = [\"<BOS>\", \"human\"]\n",
    "    if use_not:\n",
    "        tokens += [\"not\", action]\n",
    "    else:\n",
    "        tokens += [action]\n",
    "    tokens += [\"nature\", \"<EOS>\"]\n",
    "\n",
    "    # consume/destroy => collapse\n",
    "    # preserve => sustain\n",
    "    # Label: 1 = sustain, 0 = collapse\n",
    "    if action in [\"consume\", \"destroy\"]:\n",
    "        y = 0\n",
    "    else:\n",
    "        y = 1\n",
    "    \n",
    "    if use_not:\n",
    "        y = 1 - y\n",
    "    \n",
    "    return tokens, y\n",
    "\n",
    "def make_dataset(n=400):\n",
    "    X, Y = [], []\n",
    "    for _ in range(n):\n",
    "        tok, y = sample_sentence()\n",
    "        X.append(encode(tok))\n",
    "        Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "X_train, y_train = make_dataset(800)\n",
    "X_test,  y_test  = make_dataset(200)\n",
    "\n",
    "for i in range(8):\n",
    "    print(decode(X_train[i]), \"->\", \"sustain\" if y_train[i]==1 else \"collapse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac549775-ba08-4a6b-a4af-8e0de7fa47fe",
   "metadata": {},
   "source": [
    "## Padding and Batching\n",
    "\n",
    "This stage prepares variable-length text sequences for efficient training in PyTorch by combining batching and padding. The batches function first groups the dataset into mini-batches of a fixed size, optionally shuffling the data to prevent the model from learning spurious patterns related to ordering. For each mini-batch, the pad_batch function adjusts all sequences to the same length by appending a special padding token (PAD_ID, here chosen as the <EOS> token) to shorter sequences. This allows the sequences to be stacked into a rectangular tensor, which is required for efficient computation. Simultaneously, a mask is created to indicate which positions correspond to real tokens (1) and which correspond to padding (0), enabling models to ignore padded elements when necessary. The result is a set of tensors containing the padded inputs, their masks, and the corresponding labels, allowing the model to process multiple examples in parallel during training.\n",
    "\n",
    "Steps performed:\n",
    "\n",
    "1. Create a list of dataset indices.\n",
    "2. Optionally shuffle them to avoid learning order-dependent patterns.\n",
    "3. Split the indices into chunks of size `batch_size`.\n",
    "4. Select the corresponding input sequences and labels.\n",
    "5. Apply padding to the sequences in that chunk.\n",
    "6. Return tensors for inputs, masks, and labels.\n",
    "\n",
    "Example\n",
    "\n",
    "Dataset:\n",
    "```\n",
    "Sentence 1 → [0, 2, 4, 3, 1]\n",
    "Sentence 2 → [0, 2, 7, 6, 3, 1]\n",
    "Sentence 3 → [0, 2, 5, 3, 1]\n",
    "```\n",
    "\n",
    "Possible batches:\n",
    "Batch 1 → Sentence 1, Sentence 2\n",
    "Batch 2 → Sentence 3\n",
    "\n",
    "Within each batch, sequences may have different lengths. The `pad_batch` function makes them uniform by appending a special token (`PAD_ID`, here `<EOS>`) to shorter sequences.\n",
    "\n",
    "This enables stacking them into a rectangular tensor of shape **(B, T)**:\n",
    "\n",
    "- **B** = batch size  \n",
    "- **T** = maximum sequence length in that batch  \n",
    "\n",
    "Padding does not change the original content; it only adds “empty” tokens.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Batch containing three sentences:\n",
    "\n",
    "```\n",
    "[0, 2, 4, 3, 1] (length 5)\n",
    "[0, 2, 7, 6, 3, 1] (length 6)\n",
    "[0, 2, 5, 3, 1] (length 5)\n",
    "```\n",
    "\n",
    "\n",
    "Maximum length = 6 → pad shorter sequences:\n",
    "\n",
    "```\n",
    "[0, 2, 4, 3, 1, 1]\n",
    "[0, 2, 7, 6, 3, 1]\n",
    "[0, 2, 5, 3, 1, 1]\n",
    "```\n",
    "\n",
    "\n",
    "Now they can be stacked into a tensor: (3,6).\n",
    "\n",
    "### Mask Creation\n",
    "\n",
    "A mask is created alongside the padded sequences to distinguish real tokens from padding.\n",
    "\n",
    "- **1** → real token  \n",
    "- **0** → padding  \n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Sequence 1 → [1, 1, 1, 1, 1, 0]\n",
    "Sequence 2 → [1, 1, 1, 1, 1, 1]\n",
    "Sequence 3 → [1, 1, 1, 1, 1, 0]\n",
    "```\n",
    "\n",
    "\n",
    "This mask allows models (especially sequence models) to ignore padded positions during computation.\n",
    "\n",
    "\n",
    "### Final Output of a Batch\n",
    "\n",
    "Each call to `batches()` yields three tensors:\n",
    "\n",
    "- **xb** → padded input sequences of shape (B, T)  \n",
    "- **mb** → mask indicating real tokens of shape (B, T)  \n",
    "- **yb** → labels for the batch of shape (B)  \n",
    "\n",
    "These tensors enable efficient parallel processing of variable-length text data during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b48b63a-9042-43dc-90d3-58ea7d4bbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch(seqs, pad_id):\n",
    "    max_len = max(len(s) for s in seqs)\n",
    "    out = []\n",
    "    mask = []\n",
    "    for s in seqs:\n",
    "        padded = s + [pad_id]*(max_len-len(s))\n",
    "        out.append(padded)\n",
    "        mask.append([1]*len(s) + [0]*(max_len-len(s)))  \n",
    "    return torch.tensor(out, dtype=torch.long), torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "PAD_ID = stoi[\"<EOS>\"] \n",
    "\n",
    "def batches(X, Y, batch_size=64, shuffle=True):\n",
    "    idx = list(range(len(X)))\n",
    "    if shuffle:\n",
    "        random.shuffle(idx)\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        j = idx[i:i+batch_size]\n",
    "        xb, mb = pad_batch([X[k] for k in j], PAD_ID)\n",
    "        yb = torch.tensor([Y[k] for k in j], dtype=torch.long)\n",
    "        yield xb, mb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4e6cb-e8e9-4a8b-b043-3b3135761639",
   "metadata": {},
   "source": [
    "## Bag of Words \n",
    "\n",
    "This cell implements and trains a simple Bag-of-Words (BoW) text classifier using PyTorch. The bow_features function converts a batch of tokenized sequences (x_ids, shape B×T) into fixed-length vectors of size equal to the vocabulary, counting how many times each token appears in each sequence. The BoWClassifier class defines a minimal neural network consisting of a single linear layer that maps these count vectors to two output logits, corresponding to a binary classification task. The train_model function then trains this model using the Adam optimizer and cross-entropy loss over multiple epochs: for each mini-batch produced by the previously defined batching function, it performs a forward pass, computes loss, backpropagates gradients, and updates parameters. After each epoch, the model is evaluated on both training and test sets by computing accuracy in evaluation mode without gradient tracking. Finally, the code instantiates the classifier with the given vocabulary size and trains it for 20 epochs, periodically printing loss and accuracy metrics to monitor learning progress.\n",
    "\n",
    "\n",
    "### Input format: `(B, T)`\n",
    "Each batch input `x_ids` has shape:\n",
    "\n",
    "- **B** = number of sentences in the batch (batch size)\n",
    "- **T** = number of token positions per sentence (after padding)\n",
    "\n",
    "Example batch (token IDs) with a tiny vocabulary:\n",
    "\n",
    "- Vocabulary (example mapping):\n",
    "  - `0=\"<BOS>\"`, `1=\"<EOS>\"`, `2=\"human\"`, `3=\"nature\"`, `4=\"consume\"`, `5=\"preserve\"`, `6=\"destroy\"`, `7=\"not\"`\n",
    "\n",
    "Example sentences (already padded with `<EOS>` as PAD):\n",
    "\n",
    "```\n",
    "x_ids =\n",
    "[\n",
    "[0, 2, 4, 3, 1, 1], # <BOS> human consume nature <EOS> <EOS>\n",
    "[0, 2, 7, 6, 3, 1], # <BOS> human not destroy nature <EOS>\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "So here:\n",
    "\n",
    "- `B = 2`\n",
    "- `T = 6`\n",
    "\n",
    "\n",
    "\n",
    "### BoW transformation: `(B, T) → (B, V)`\n",
    "The function `bow_features(x_ids, vocab_size)` creates a matrix `bow` of shape:\n",
    "\n",
    "- **V** = vocabulary size\n",
    "\n",
    "For each sentence, it counts how many times each token ID appears.\n",
    "\n",
    "#### Example BoW output\n",
    "\n",
    "If `V = 8`, then each sentence becomes a length-8 vector:\n",
    "\n",
    "- Sentence 1: `[0,2,4,3,1,1]` contains:\n",
    "  - `<BOS>` (0): 1 time\n",
    "  - `<EOS>` (1): 2 times\n",
    "  - `human` (2): 1 time\n",
    "  - `nature` (3): 1 time\n",
    "  - `consume` (4): 1 time\n",
    "\n",
    "So its BoW vector is:\n",
    "\n",
    "```\n",
    "bow[0] = [1, 2, 1, 1, 1, 0, 0, 0]\n",
    "```\n",
    "\n",
    "- Sentence 2: `[0,2,7,6,3,1]` contains:\n",
    "  - `<BOS>`: 1\n",
    "  - `<EOS>`: 1\n",
    "  - `human`: 1\n",
    "  - `nature`: 1\n",
    "  - `destroy`: 1\n",
    "  - `not`: 1\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "bow[1] = [1, 1, 1, 1, 0, 0, 1, 1]\n",
    "```\n",
    "This means:\n",
    "\n",
    "Input: BoW vector of size V\n",
    "\n",
    "Output: 2 logits (one per class)\n",
    "\n",
    "So the forward pass is:\n",
    "\n",
    "Convert tokens to BoW counts:\n",
    "\n",
    "x = bow_features(x_ids, vocab_size) → shape (B, V)\n",
    "\n",
    "Compute logits:\n",
    "\n",
    "logits = W x + b → shape (B, 2)\n",
    "\n",
    "\n",
    "## NN \n",
    "\n",
    "Even though the model is written as a PyTorch `nn.Module`, it is **not deep**. The architecture contains:\n",
    "\n",
    "- **No hidden layers**\n",
    "- **No nonlinear activations** (e.g., ReLU, tanh)\n",
    "- Only a single linear transformation from features to class scores\n",
    "\n",
    "That makes it equivalent to **multinomial logistic regression** (also called **softmax regression**) with 2 classes.\n",
    "\n",
    "### Linear scoring (logits)\n",
    "\n",
    "After converting each sentence to a Bag-of-Words vector $(x \\in \\mathbb{R}^{V})$, the linear layer computes the **logits**:\n",
    "\n",
    "$$\n",
    "z = Wx + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $W \\in \\mathbb{R}^{2 \\times V}$ is the weight matrix (one row per class)\n",
    "- $b \\in \\mathbb{R}^{2}$ is the bias vector\n",
    "- $z \\in \\mathbb{R}^{2}$ are the logits $z_0, z_1$\n",
    "\n",
    "For a batch of size \\(B\\), this becomes:\n",
    "\n",
    "- $X \\in \\mathbb{R}^{B \\times V}$\n",
    "- $Z = XW^\\top + b \\in \\mathbb{R}^{B \\times 2}$\n",
    "\n",
    "\n",
    "\n",
    "### From logits to probabilities (softmax)\n",
    "\n",
    "The logits are turned into class probabilities via the **softmax** function:\n",
    "\n",
    "$$\n",
    "p(y=k \\mid x) = \\frac{e^{z_k}}{\\sum_{j=0}^{1} e^{z_j}}\n",
    "\\quad \\text{for } k \\in \\{0,1\\}.\n",
    "$$\n",
    "\n",
    "This produces two probabilities that sum to 1:\n",
    "\n",
    "$$\n",
    "p(y=0 \\mid x) + p(y=1 \\mid x) = 1.\n",
    "$$\n",
    "\n",
    "### Binary logistic regression equivalence\n",
    "\n",
    "With two classes, you can also express this as a single sigmoid on the logit difference:\n",
    "\n",
    "$$\n",
    "p(y=1 \\mid x) = \\sigma(z_1 - z_0) = \\frac{1}{1 + e^{-(z_1 - z_0)}}.\n",
    "$$\n",
    "\n",
    "This is why a 2-class softmax classifier is closely related to standard binary logistic regression.\n",
    "\n",
    "\n",
    "\n",
    "## 5) Loss function: cross-entropy\n",
    "\n",
    "During training, the code uses:\n",
    "\n",
    "```python\n",
    "loss = F.cross_entropy(logits, yb)\n",
    "```\n",
    "\n",
    "For one example $(x, y)$, cross-entropy is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x,y) = -\\log\\left(p(y \\mid x)\\right).\n",
    "$$\n",
    "\n",
    "For a batch of size \\(B\\):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{batch} = \\frac{1}{B}\\sum_{i=1}^{B} -\\log\\left(p(y_i \\mid x_i)\\right).\n",
    "$$\n",
    "\n",
    "Minimizing cross-entropy encourages the model to assign high probability to the correct class.\n",
    "\n",
    "\n",
    "\n",
    "## 6) Training loop: what happens each epoch\n",
    "\n",
    "Each epoch repeats the following steps:\n",
    "\n",
    "1. **Training phase**\n",
    "   - Iterate through batches from `batches(X_train, y_train, batch_size=64)`\n",
    "   - For each batch:\n",
    "     - Compute logits: `logits = model(xb)`\n",
    "     - Compute loss: `loss = cross_entropy(logits, yb)`\n",
    "     - Backpropagate gradients: `loss.backward()`\n",
    "     - Update parameters with Adam: `opt.step()`\n",
    "\n",
    "2. **Evaluation phase**\n",
    "   - Switch to evaluation mode: `model.eval()`\n",
    "   - Compute accuracy on train and test sets using `argmax` on logits:\n",
    "     $$\n",
    "     \\hat{y} = \\arg\\max_k z_k\n",
    "     $$\n",
    "\n",
    "\n",
    "\n",
    "### Concrete transformation example (end-to-end)\n",
    "\n",
    "Consider two sentences (already tokenized and padded):\n",
    "\n",
    "- Sentence A: `<BOS> human consume nature <EOS> <EOS>`\n",
    "- Sentence B: `<BOS> human not destroy nature <EOS>`\n",
    "\n",
    "Token IDs (example):\n",
    "\n",
    "```\n",
    "A = [0, 2, 4, 3, 1, 1]\n",
    "B = [0, 2, 7, 6, 3, 1]\n",
    "```\n",
    "\n",
    "### Step 1 — BoW counts \\((B,T) to (B,V)\\)\n",
    "\n",
    "If \\(V = 10\\), each sentence becomes a length-10 count vector. For Sentence A:\n",
    "\n",
    "- `<BOS>` appears 1 time\n",
    "- `human` appears 1 time\n",
    "- `consume` appears 1 time\n",
    "- `nature` appears 1 time\n",
    "- `<EOS>` appears 2 times\n",
    "\n",
    "So \\(x_A\\) looks like:\n",
    "\n",
    "```\n",
    "x_A = [1, 2, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "Sentence B becomes:\n",
    "\n",
    "```\n",
    "x_B = [1, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
    "```\n",
    "\n",
    "### Step 2 — Linear layer \\((B,V) \\to (B,2)\\)\n",
    "\n",
    "For each sentence:\n",
    "\n",
    "$$\n",
    "z = Wx + b\n",
    "$$\n",
    "\n",
    "The output \\(z\\) has two logits, one per class, e.g.:\n",
    "\n",
    "```\n",
    "z_A = [z0, z1]\n",
    "z_B = [z0, z1]\n",
    "```\n",
    "\n",
    "### Step 3 — Softmax probabilities\n",
    "\n",
    "$$\n",
    "p(y=k \\mid x) = \\frac{e^{z_k}}{e^{z_0}+e^{z_1}}\n",
    "$$\n",
    "\n",
    "The model predicts the class with the highest logit (or highest probability):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k z_k\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Interpreting the learned weights\n",
    "\n",
    "Printing:\n",
    "\n",
    "```python\n",
    "print(bow_model.lin.weight)  # shape (2, V)\n",
    "print(bow_model.lin.bias)    # shape (2,)\n",
    "```\n",
    "\n",
    "gives a weight for each **token** and each **class**.\n",
    "\n",
    "- If a token has a **large positive weight** for the \"sustain\" class, its presence increases the logit \\(z_{\\text{sustain}}\\).\n",
    "- If it has a **large negative weight**, it pushes the prediction away from that class.\n",
    "\n",
    "Because BoW uses counts, repeated tokens contribute linearly: seeing a token twice roughly doubles its contribution to the logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e971638-0968-4663-b5bd-cf2f60dc0cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 01 | loss 8.619 | acc train 0.537 | acc test 0.585\n",
      "ep 02 | loss 8.103 | acc train 0.667 | acc test 0.705\n",
      "ep 05 | loss 7.630 | acc train 0.885 | acc test 0.895\n",
      "ep 10 | loss 7.693 | acc train 0.885 | acc test 0.895\n",
      "ep 20 | loss 7.683 | acc train 0.885 | acc test 0.895\n"
     ]
    }
   ],
   "source": [
    "def bow_features(x_ids, vocab_size):\n",
    "    # x_ids: (B,T)\n",
    "    # returns count per token: (B,V)\n",
    "    B, T = x_ids.shape\n",
    "    bow = torch.zeros(B, vocab_size)\n",
    "    for b in range(B):\n",
    "        for t in range(T):\n",
    "            bow[b, x_ids[b,t]] += 1.0\n",
    "    return bow\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(vocab_size, 2)\n",
    "    def forward(self, x_ids):\n",
    "        x = bow_features(x_ids, vocab_size)\n",
    "        return self.lin(x)\n",
    "\n",
    "def train_model(model, epochs=20, lr=1e-2):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, mb, yb in batches(X_train, y_train, batch_size=64):\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # eval\n",
    "        model.eval()\n",
    "        def acc(X, Y):\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for xb, mb, yb in batches(X, Y, batch_size=128, shuffle=False):\n",
    "                with torch.no_grad():\n",
    "                    pred = model(xb).argmax(dim=1)\n",
    "                correct += (pred==yb).sum().item()\n",
    "                total += len(yb)\n",
    "            return correct/total\n",
    "        \n",
    "        if ep in [1,2,5,10,20] or ep==epochs:\n",
    "            print(f\"ep {ep:02d} | loss {total_loss:.3f} | acc train {acc(X_train,y_train):.3f} | acc test {acc(X_test,y_test):.3f}\")\n",
    "\n",
    "bow_model = BoWClassifier(vocab_size)\n",
    "train_model(bow_model, epochs=20, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad0a30dc-5611-4ad8-827c-3c77201f4c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0813,  0.3918, -0.3392, -0.3117,  0.1514, -0.3765,  0.3929, -0.2086,\n",
      "         -0.0281,  0.0837],\n",
      "        [-0.0166, -0.2844, -0.2231, -0.1305, -0.4036,  0.4730, -0.2742,  0.6491,\n",
      "         -0.2144, -0.1377]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0359, 0.3416], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(bow_model.lin.weight)\n",
    "print(bow_model.lin.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75144e8-dcf6-4ef3-a674-a455fe806939",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The model achieved reasonably good performance, with training and test accuracies approaching high values (e.g., around 0.87–0.88). This indicates that the classifier successfully learned meaningful correlations between individual words and the target labels. In this dataset, certain tokens strongly signal the outcome — for example, words like **\"preserve\"** tend to indicate *sustain*, while **\"consume\"** or **\"destroy\"** tend to indicate *collapse*. Because the Bag-of-Words representation captures word frequencies, the model can learn these associations effectively.\n",
    "\n",
    "However, the accuracy does not reach perfect performance because the BoW representation fundamentally ignores **word order, syntax, and semantic interactions** between tokens. In particular, it cannot properly model **negation**. The word **\"not\"** flips the meaning of the action (e.g., “not destroy” should imply sustain), but BoW only counts tokens independently. As a result, the sentences:\n",
    "\n",
    "```\n",
    "human destroy nature\n",
    "human not destroy nature\n",
    "```\n",
    "\n",
    "produce very similar feature vectors — the second simply adds one extra count for “not.” The model must infer the effect of negation indirectly from weights, without any notion that “not” modifies the verb that follows.\n",
    "\n",
    "More generally, BoW lacks mechanisms for:\n",
    "\n",
    "- **Attention** — it cannot focus on relationships between specific words\n",
    "- **Compositional semantics** — it cannot combine meanings across tokens\n",
    "- **Word order awareness** — it treats sentences as unordered bags of tokens\n",
    "- **Contextual understanding** — it cannot represent that one word changes another’s meaning\n",
    "\n",
    "Modern NLP models such as recurrent networks, convolutional sequence models, and especially Transformers address these limitations by modeling token interactions and context. In particular, attention mechanisms allow the model to learn that “not” is strongly connected to the verb that follows, enabling correct interpretation of negation.\n",
    "\n",
    "Therefore, while the BoW logistic regression model provides a strong baseline and demonstrates that simple lexical signals carry predictive power, its performance is inherently limited by its inability to capture structure and meaning beyond individual word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d166df4-ac44-4956-8dad-da639bdda6cb",
   "metadata": {},
   "source": [
    "## Embedding + Feedforward Neural Network (FNN)\n",
    "\n",
    "This model replaces the Bag-of-Words representation with a **learned embedding layer** followed by a **feedforward neural network (FNN)**. Unlike BoW, which only counts tokens, this architecture preserves token positions and learns dense vector representations that capture semantic similarities.\n",
    "\n",
    "\n",
    "### Word Embedding\n",
    "\n",
    "An embedding maps each discrete token ID to a continuous vector in $\\mathbb{R}^d$.\n",
    "\n",
    "Formally, an embedding layer is a lookup table:\n",
    "\n",
    "$$\n",
    "E \\in \\mathbb{R}^{V \\times d}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $V$ = vocabulary size  \n",
    "- $d$ = embedding dimension  \n",
    "- Row $E_i$ is the vector representation of token $i$\n",
    "\n",
    "For a token ID $t$, the embedding output is:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_t = E[t] \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "If $d = 8$, the word `\"destroy\"` might map to:\n",
    "\n",
    "```\n",
    "destroy → [0.21, -0.44, 0.18, 0.03, 0.91, -0.12, 0.07, 0.30]\n",
    "```\n",
    "\n",
    "Semantically related words tend to get similar vectors during training.\n",
    "\n",
    "\n",
    "\n",
    "### Input Processing (Fixed Length)\n",
    "\n",
    "The model expects sequences of length `max_len`. Therefore, each batch is adjusted as follows:\n",
    "\n",
    "- If a sequence is longer than `max_len`, it is truncated.\n",
    "- If shorter, it is padded with the padding token ID.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\n",
    "x \\in \\mathbb{N}^{B \\times T} \\rightarrow x' \\in \\mathbb{N}^{B \\times L}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $B$ = batch size  \n",
    "- $T$ = original length  \n",
    "- $L = \\text{max\\_len}$  \n",
    "\n",
    "\n",
    "### Embedding Lookup\n",
    "\n",
    "After padding/truncation, the embedding layer converts token IDs into vectors:\n",
    "\n",
    "$$\n",
    "x' \\in \\mathbb{N}^{B \\times L}\n",
    "\\quad \\rightarrow \\quad\n",
    "E(x') \\in \\mathbb{R}^{B \\times L \\times d}\n",
    "$$\n",
    "\n",
    "This tensor contains one $d$-dimensional vector per token position.\n",
    "\n",
    "#### Example\n",
    "\n",
    "For $L = 6$, $d = 8$:\n",
    "\n",
    "```\n",
    "(B, 6) → (B, 6, 8)\n",
    "```\n",
    "\n",
    "Each sentence becomes a matrix of embeddings.\n",
    "\n",
    "\n",
    "\n",
    "### Flattening for FNN Input\n",
    "\n",
    "Feedforward networks expect a single vector per example, so the embeddings are flattened:\n",
    "\n",
    "$$\n",
    "\\mathbb{R}^{B \\times L \\times d} \\rightarrow \\mathbb{R}^{B \\times (L \\cdot d)}\n",
    "$$\n",
    "\n",
    "This operation concatenates all token embeddings into one long vector.\n",
    "\n",
    "#### Example\n",
    "\n",
    "If $L = 6$ and $d = 8$:\n",
    "\n",
    "$$\n",
    "L \\cdot d = 48\n",
    "$$\n",
    "\n",
    "So each sentence becomes a vector in $\\mathbb{R}^{48}$.\n",
    "\n",
    "\n",
    "\n",
    "### Feedforward Neural Network (FNN)\n",
    "\n",
    "A feedforward neural network applies one or more fully connected layers:\n",
    "\n",
    "#### First layer\n",
    "\n",
    "$$\n",
    "h = \\tanh(W_1 x + b_1)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $W_1 \\in \\mathbb{R}^{H \\times (L d)}$\n",
    "- $H$ = number of hidden units\n",
    "- $\\tanh(\\cdot)$ introduces nonlinearity\n",
    "\n",
    "#### Second (output) layer\n",
    "\n",
    "$$\n",
    "z = W_2 h + b_2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $W_2 \\in \\mathbb{R}^{2 \\times H}$\n",
    "- $z \\in \\mathbb{R}^{2}$ are logits for the two classes\n",
    "\n",
    "\n",
    "\n",
    "### Classification via Softmax\n",
    "\n",
    "Probabilities are obtained with softmax:\n",
    "\n",
    "$$\n",
    "p(y=k \\mid x) =\n",
    "\\frac{e^{z_k}}{\\sum_{j=0}^{1} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Prediction:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_k z_k\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### What This Model Learns\n",
    "\n",
    "Compared to BoW, this architecture can learn:\n",
    "\n",
    "- Position-sensitive patterns (because embeddings are ordered)\n",
    "- Nonlinear combinations of features\n",
    "- Interactions between tokens\n",
    "- Semantic similarity between words\n",
    "\n",
    "However, because the embeddings are flattened, the model still does not explicitly model long-range structure or token-to-token relationships beyond fixed positions.\n",
    "\n",
    "\n",
    "\n",
    "### End-to-End Example\n",
    "\n",
    "Consider the padded sentence:\n",
    "\n",
    "```\n",
    "<BOS> human not destroy nature <EOS>\n",
    "```\n",
    "\n",
    "Token IDs:\n",
    "\n",
    "```\n",
    "[0, 2, 7, 6, 3, 1]\n",
    "```\n",
    "\n",
    "#### Step 1 — Embedding lookup\n",
    "\n",
    "Each ID becomes a vector:\n",
    "\n",
    "```\n",
    "(6) → (6 × 8)\n",
    "```\n",
    "\n",
    "#### Step 2 — Flatten\n",
    "\n",
    "```\n",
    "(6 × 8) → (48)\n",
    "```\n",
    "\n",
    "#### Step 3 — Hidden layer\n",
    "\n",
    "Apply nonlinear transformation:\n",
    "\n",
    "$$\n",
    "h = \\tanh(W_1 x + b_1)\n",
    "$$\n",
    "\n",
    "#### Step 4 — Output layer\n",
    "\n",
    "Compute logits:\n",
    "\n",
    "$$\n",
    "z = W_2 h + b_2\n",
    "$$\n",
    "\n",
    "#### Step 5 — Prediction\n",
    "\n",
    "Softmax → class probabilities → final label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "551a8ec1-4196-44e8-98f6-eee979de200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 01 | loss 7.319 | acc train 0.885 | acc test 0.895\n",
      "ep 02 | loss 3.543 | acc train 1.000 | acc test 1.000\n",
      "ep 05 | loss 0.145 | acc train 1.000 | acc test 1.000\n",
      "ep 10 | loss 0.030 | acc train 1.000 | acc test 1.000\n",
      "ep 20 | loss 0.009 | acc train 1.000 | acc test 1.000\n",
      "ep 25 | loss 0.006 | acc train 1.000 | acc test 1.000\n"
     ]
    }
   ],
   "source": [
    "class EmbFNN(nn.Module):\n",
    "    def __init__(self, vocab_size, d=8, hidden=32, max_len=6):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d)\n",
    "        self.max_len = max_len\n",
    "        self.fc1 = nn.Linear(max_len*d, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, 2)\n",
    "    def forward(self, x_ids):\n",
    "\n",
    "        x = x_ids[:, :self.max_len]\n",
    "        if x.shape[1] < self.max_len:\n",
    "            pad = torch.full((x.shape[0], self.max_len-x.shape[1]), PAD_ID, dtype=torch.long)\n",
    "            x = torch.cat([x, pad], dim=1)\n",
    "        e = self.emb(x)                 # (B,T,d)\n",
    "        h = e.reshape(e.shape[0], -1)   # (B,T*d)\n",
    "        h = torch.tanh(self.fc1(h))\n",
    "        return self.fc2(h)\n",
    "\n",
    "fnn = EmbFNN(vocab_size, d=8, hidden=32, max_len=6)\n",
    "train_model(fnn, epochs=25, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393167bc-e5e7-4c69-8dec-454edf589800",
   "metadata": {},
   "source": [
    "Compared to the Bag-of-Words linear classifier, the embedding-based model represents a significant conceptual and practical advance. In the BoW approach, each sentence is reduced to token counts, discarding word order and treating all words as independent features; as a result, the model can only learn shallow correlations between the presence of individual tokens and the target class. By contrast, an embedding layer maps each token to a dense continuous vector that is learned during training, allowing the model to capture semantic relationships between words and to represent similar concepts with nearby vectors in a geometric space. When these embeddings are processed by a non-linear neural network, the model can learn interactions between words and their positions, effectively modeling simple compositional rules rather than isolated features. In practice, this means the classifier can begin to distinguish patterns such as how negation modifies meaning or how certain combinations of words imply different outcomes — capabilities that are fundamentally inaccessible to a pure Bag-of-Words model. Thus, embeddings transform the problem from counting symbols to reasoning over structured representations, enabling the network to approximate underlying linguistic rules instead of memorizing surface statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00338d43-81f5-45fb-8465-5febada5fe8b",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "\n",
    "A Recurrent Neural Network (RNN) is a neural architecture designed specifically to process sequential data. Unlike standard feed-forward neural networks (FNNs), which treat inputs as independent fixed-size vectors, an RNN processes one element of a sequence at a time while maintaining an internal memory of previous elements. This memory is represented by a hidden state that evolves across time steps.\n",
    "\n",
    "Formally, given an input sequence $x_1, x_2, \\dots, x_T$, an RNN computes hidden states according to\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "where $h_{t-1}$ carries information from the past. This recurrence allows the model to capture dependencies across tokens and model how earlier words influence later ones.\n",
    "\n",
    "In contrast, a standard neural network with flattened inputs ignores the temporal structure of language. When embeddings are flattened into a single vector, the model sees only a fixed arrangement of features, without an explicit notion of order propagation. An RNN preserves sequence structure by updating its hidden state at each time step, effectively accumulating contextual information as it reads the sentence.\n",
    "\n",
    "Another important difference is parameter sharing. The same transformation is applied at every time step, enabling the model to process sequences of variable length and generalize patterns across positions. This makes RNNs suitable for language, time series, and other sequential tasks.\n",
    "\n",
    "In this experiment, the embedded sentence is processed token by token, and the final hidden state is used as a compact representation of the entire sequence for classification. However, basic RNNs may struggle with long-range dependencies due to vanishing or exploding gradients, which motivated more advanced architectures such as LSTM, GRU, and Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3de376cd-cd6d-4c47-b0a2-5321e7b102ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 01 | loss 8.419 | acc train 0.782 | acc test 0.810\n",
      "ep 02 | loss 5.079 | acc train 1.000 | acc test 1.000\n",
      "ep 05 | loss 0.067 | acc train 1.000 | acc test 1.000\n",
      "ep 10 | loss 0.023 | acc train 1.000 | acc test 1.000\n",
      "ep 20 | loss 0.009 | acc train 1.000 | acc test 1.000\n",
      "ep 25 | loss 0.006 | acc train 1.000 | acc test 1.000\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, d=8, h=24):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d)\n",
    "        self.rnn = nn.RNN(input_size=d, hidden_size=h, batch_first=True, nonlinearity=\"tanh\")\n",
    "        self.fc = nn.Linear(h, 2)\n",
    "    def forward(self, x_ids):\n",
    "        e = self.emb(x_ids)         # (B,T,d)\n",
    "        out, hn = self.rnn(e)       # (B,T,h), hn (1,B,h)\n",
    "        last = hn[0]                # (B,h)\n",
    "        return self.fc(last)\n",
    "\n",
    "rnn = SimpleRNN(vocab_size, d=8, h=24)\n",
    "train_model(rnn, epochs=25, lr=3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75be35-f17f-4123-866b-8132b081ae86",
   "metadata": {},
   "source": [
    "The RNN achieves perfect training and test accuracy after a few epochs, indicating that it successfully learns the underlying patterns in the dataset. Compared to the embedding-based feed-forward network (EmbFNN), the RNN benefits from explicitly modeling the sequential structure of the sentence rather than relying on a flattened representation. While the EmbFNN can capture interactions between word embeddings and their fixed positions, it still treats the input as a static vector. The RNN, on the other hand, processes tokens step by step and accumulates contextual information in its hidden state, allowing it to represent how earlier words influence later ones. In this simple task, both models eventually reach perfect performance because the classification rule is relatively easy to learn. However, the RNN does so in a way that is more consistent with the compositional nature of language, making it better suited for generalizing to longer or more complex sequences. Thus, the results illustrate that incorporating sequential memory can improve representational power even when overall accuracy appears similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da7688-aaa2-4450-8a8a-2f5af37199c3",
   "metadata": {},
   "source": [
    "## Mini Transformer (Self-Attention Model)\n",
    "\n",
    "The Transformer architecture represents a major shift from recurrent models by replacing sequential processing with self-attention. Instead of reading tokens one by one, the model processes the entire sequence simultaneously and computes pairwise interactions between all tokens. Each input token is first converted into a vector through an embedding layer and augmented with positional embeddings, allowing the model to retain information about word order despite operating in parallel.\n",
    "\n",
    "Self-attention is implemented through three learned projections of the input: queries $Q$, keys $K$, and values $V$. Attention scores are computed as\n",
    "\n",
    "$$\n",
    "A = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)\n",
    "$$\n",
    "\n",
    "which produces a matrix of weights describing how strongly each token attends to every other token. These weights are then used to aggregate information from the value vectors, yielding context-aware representations in which each position incorporates information from the entire sentence.\n",
    "\n",
    "The model includes residual connections and layer normalization, which stabilize training and allow deeper representations to be learned. A position-wise feed-forward network further transforms each token representation independently, increasing expressive power. For classification, the representation of the first token is used as a pooled summary of the sequence.\n",
    "\n",
    "Compared to feed-forward networks and RNNs, the Transformer can model long-range dependencies more effectively because information does not need to propagate step by step. Every token can directly interact with every other token in a single layer. This makes attention-based models highly expressive and scalable, forming the foundation of modern large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dada8f1a-33ba-409f-bf94-186a17075956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 01 | loss 8.763 | acc train 0.885 | acc test 0.895\n",
      "ep 02 | loss 5.716 | acc train 0.885 | acc test 0.895\n",
      "ep 05 | loss 3.104 | acc train 0.885 | acc test 0.895\n",
      "ep 10 | loss 0.042 | acc train 1.000 | acc test 1.000\n",
      "ep 20 | loss 0.013 | acc train 1.000 | acc test 1.000\n",
      "ep 30 | loss 0.006 | acc train 1.000 | acc test 1.000\n"
     ]
    }
   ],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d=16, max_len=6):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.max_len = max_len\n",
    "        self.emb = nn.Embedding(vocab_size, d)\n",
    "        self.pos = nn.Embedding(max_len, d)\n",
    "        \n",
    "        # Wq, Wk, Wv\n",
    "        self.Wq = nn.Linear(d, d, bias=False)\n",
    "        self.Wk = nn.Linear(d, d, bias=False)\n",
    "        self.Wv = nn.Linear(d, d, bias=False)\n",
    "        self.out = nn.Linear(d, d, bias=False)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d, 4*d),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d, d)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d)\n",
    "        self.norm2 = nn.LayerNorm(d)\n",
    "        self.cls = nn.Linear(d, 2)\n",
    "        \n",
    "        self.last_attn = None  \n",
    "\n",
    "    def forward(self, x_ids, mask=None):\n",
    "        x = x_ids[:, :self.max_len]\n",
    "        if x.shape[1] < self.max_len:\n",
    "            pad = torch.full((x.shape[0], self.max_len-x.shape[1]), PAD_ID, dtype=torch.long)\n",
    "            x = torch.cat([x, pad], dim=1)\n",
    "        B, T = x.shape\n",
    "        \n",
    "        pos_ids = torch.arange(T).unsqueeze(0).repeat(B,1)\n",
    "        X = self.emb(x) + self.pos(pos_ids)  # (B,T,d)\n",
    "        \n",
    "        Q = self.Wq(X)\n",
    "        K = self.Wk(X)\n",
    "        Vv = self.Wv(X)\n",
    "        \n",
    "        scores = (Q @ K.transpose(1,2)) / math.sqrt(self.d)  # (B,T,T)\n",
    "        \n",
    "        \n",
    "        if mask is not None:\n",
    "            m = mask[:, :T]\n",
    "            if m.shape[1] < T:\n",
    "                m = torch.cat([m, torch.zeros(B, T-m.shape[1])], dim=1)\n",
    "            # m: (B,T) -> (B,1,T)\n",
    "            scores = scores.masked_fill((m.unsqueeze(1) == 0), -1e9)\n",
    "        \n",
    "        A = torch.softmax(scores, dim=-1)   # (B,T,T)\n",
    "        self.last_attn = A.detach().cpu()\n",
    "        \n",
    "        Z = A @ Vv                           # (B,T,d)\n",
    "        Z = self.out(Z)\n",
    "        \n",
    "        X2 = self.norm1(X + Z)\n",
    "        Ff = self.ffn(X2)\n",
    "        X3 = self.norm2(X2 + Ff)\n",
    "        \n",
    "\n",
    "        pooled = X3[:,0,:]\n",
    "        return self.cls(pooled)\n",
    "\n",
    "tr = MiniTransformer(vocab_size, d=16, max_len=6)\n",
    "\n",
    "def train_transformer(model, epochs=30, lr=3e-3):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, mb, yb in batches(X_train, y_train, batch_size=64):\n",
    "            logits = model(xb, mask=mb)\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        def acc(X, Y):\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for xb, mb, yb in batches(X, Y, batch_size=128, shuffle=False):\n",
    "                with torch.no_grad():\n",
    "                    pred = model(xb, mask=mb).argmax(dim=1)\n",
    "                correct += (pred==yb).sum().item()\n",
    "                total += len(yb)\n",
    "            return correct/total\n",
    "        \n",
    "        if ep in [1,2,5,10,20,30] or ep==epochs:\n",
    "            print(f\"ep {ep:02d} | loss {total_loss:.3f} | acc train {acc(X_train,y_train):.3f} | acc test {acc(X_test,y_test):.3f}\")\n",
    "\n",
    "train_transformer(tr, epochs=30, lr=3e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf311]",
   "language": "python",
   "name": "conda-env-tf311-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
